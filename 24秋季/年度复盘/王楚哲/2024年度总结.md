# 2024年度总结

## 一. 本学期做过的事情
### 1. 基础铺垫
**知识类：**

**· markdownd的基础语法格式**

**·C++基础：语句字符串指针数组**

**·基础数据结构：数组队列链表栈**

**工具类：**

**必要的工具和环境：anaconda，pytorch，cuda，git，kaggle数据集，zotero和论文翻译插件**

### 2. 阅读过的论文

**1. 《Attention  is all you need》：一个基本的transformer模型，包含一个编码器和解码器，通过对每个token进行张量信息嵌入，再利用QKV三个矩阵来进行上下文关联计算 ，最后利用解码器解码，完成了一种有效的“猜词”系统**

**2. 《Galaxy: A Resource-Efficient Collaborative Edge  AI System for In-situ Transformer Inference》：ai计算具有资源密集和数据量庞大的特性，本文提出了一种利用闲置边缘设备进行异构化计算的系统设计：1）设计了一种HMP架构，进行多设备协同transformer推理 2）设计了一个并行规划器，以负载均衡的方式进行工作分配**
### 3. 做过的项目（目前在项目式学习上具有一定的迷茫性）

**基础：mnist手写数字识别（依托matlab）：matlab的神经网络集成性和便捷性使得我可以很快速的通过图标构建训练一个神经网络**

**疑惑：如何能够找到论文里面类似的项目和数据集：例如是否有根据galaxy构建的项目，我又应该怎么具体去在项目钟学习**

---
## 二.实质性的进步
### 1. 掌握了论文阅读的方法
**先下载论文的双语翻译，然后按照剥洋葱法则去阅读文章：**
**1）文章的大体目标：这篇文章针对什么方面的什么问题，提出了什么样子的解决方案** 

**2）文章的解决方案：这篇文章的解决方案的架构是什么，如果是一个系统的话又分为哪些子系统，每个子系统的作用是什么，子系统的架构又是什么**

**3）具体的功能实现：每一个功能的具体表现是什么（什么样的神经网络），它的具体数学理论是啥，它的工作机理是啥，它的代码实现是啥**
### 2.提高了主动搜集信息的能力
**1）碰到不会的事情：先想明白自己的疑惑出在哪个方面，再根据对口的方面的清晰性选择问ai和问搜索引擎**

**2）如果一个问题能够揭露很多的底层问题，我会选择先记录下这个问题牵扯到哪些底层原理，然后立刻停止继续深入追问，现阶段还是要以广度的学习为主，避免在小细节纠结太久却忽略了主要问题**

-----
## 三.未来的打算
### **1.论文精读**
**1）由于我的能力有限，目前的论文阅读只能进行到大致了解文章的解决方案，对具体的架构图和代码实现的了解和掌握远远不够**

**2）我计划寒假反复阅读attention和galaxy两篇论文，对attention的注意力机制完善到能够自主写出代码的程度，然后能够明白galaxy的资源分配过程，搞清楚论文钟画的图**

### **2.基础学习**

**1）深刻了解到自己的线性代数基础还是不够支撑我看懂各种张量和矩阵的计算过程**

**2）概率论基础的确实使我难以了解到神经网络钟的各种函数和分布，需要通过学习来知其所以然**